Consider executing Join[i=j](R,S) with parameters:
* rR = 1000,  cR = 20,  bR = 50
* rS = 3000,  cS = 20,  bS = 150
* R.i is primary key, and S has index on S.j
* S is sorted on S.j, each R tuple joins with 2 S tuples
* DBMS has N = 42 buffers available for the join
* data + hash have reasonably uniform distribution
* 1 in-memory partition

Method:
similar to grace hash join,
except that one partition of R is held in memory

Need to allocate buffers to tasks:

For phase 1 and 2:
* 1 input buffer + 1 output buffer
* k total partitions
* p buffers for in-memory partition
* partition size = bR/k

For phase 3:
* 1 input buffer + 1 output buffer
* 40 buffers for hash table for each partition


Phase 1 possibilities ...

For all scenarios, have
* 1 input buffer
* 1 unused buffer (required for output in phase 2)
* 40 buffers available for partitions

For k=10 partitions:
* partition-size = ceil(50/10) = 5 pages/partition
* 5 for partition[0],  9 for other 9 partitions
* leaving 26 not used, so add to hash table for partition[0]
* p = 31

For k=8 partitions:
* partition-size = ceil(50/8) = 7 pages/partition
* 7 for partition[0],  7 for other 7 partitions
* leaving 26 not used, so add to hash table for partition[0]
* p = 33

For k=6 partitions:
* partition-size = ceil(50/6) = 9 pages/partition
* 9 for partition[0],  5 for other 5 partitions
* leaving 26 not used, so add to hash table for partition[0]
* p = 35

For k=3 partitions:
* partition-size = ceil(50/3) = 17 pages/partition
* 17 for partition[0],  2 for other 2 partitions
* leaving 21 not used, so add to hash table for partition[0]
* p = 38

Using "excess" buffers for partition 0 hopefully eliminates overflows

So, choose 8 partitions
* 1 input buffer, 1 output buffer
* 7 output buffers for partitions 1..7
* use remaining 33 for partition 0
* h2() maps into 0 .. 32

Assuming uniform distribution,
* tuples/R-partition = ceil(rR/8) = 125
* pages/R-partition = ceil(125/cR) = 7 
* tuples/S-partition = ceil(rS/8) = 375
* pages/S-partition = ceil(125/cR) = 19

Cost:
reminder
* rR = 1000,  cR = 20,  bR = 50
* rS = 3000,  cS = 20,  bS = 150

Factors in cost:
(a) read whole R table
(b) using:
    - 1 buffer for input,
    - 7 output buffers to make other partitions [1]..[7]
(c) assuming uniform distribution:
    - each partition of R has 125 tuples
    - each disk partition needs 7 pages  (ceil(125/20))
(d) read whole S table
    - for tuples with hash [0],
      join with tuples in memory partition [0]
    - for tuples with hash [1]..[7], write to disk partitions
    - each S-partition has 375 tuples => 19 pages
(e) ~ 1/8 of S tuples are joined, rest are written to disk
(f) using:
    - 1 buffer for input, 1 buffer for output
    - 40 buffers for in-memory hash table
(f) for i in [1]..[7]
       read in R-partition [i], hash into memory buffers
          (partition has 125 tuples => approx 3 per buffer!)
       scan S-partition [i], use hash and examine one buffer

Costs: (a) bR=50 reads, (b) 7*7 writes, (d) bS=150 reads,
       (e) 7*19 writes, (f) 7*7 reads (g) 7*19 reads

#pages
= 50 + 49 + 150 + 133 + 49 + 133
= 564
(a little better than 600 for grace hash join)


Exercise:

Try different k values to see how much it can be improved


From comments above ...
each S tuple is compared against ~3 R tuples

#checks
= rS * 3 = 3000 * 3 = 9000

(worse than 6000 for grace hash join)
